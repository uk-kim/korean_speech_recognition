{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for KOREAN Datasets\n",
    ". [AIHub](http://www.aihub.or.kr/aidata/105)에서 제공되는 데이터셋에 대한 전처리 및 가공 단계를 다루는 과정입니다.\n",
    ". 이 문서에서 포함하는 내용은 다음과 같습니다.\n",
    "  - Data Load                    (O)\n",
    "  - Transcript 전처리 및 Label 생성 (O)\n",
    "  - 오디오 전처리(MFCC) 및 저장       (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load\n",
    "- Audio File : 16k / PCM / 16bit LE\n",
    "- Transcript File : EUC-KR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ext = \".pcm\"\n",
    "trans_ext = \".txt\"\n",
    "\n",
    "data_dir = \"../data/KsponSpeech_sample/\"\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "ENDIAN = \"int16\"\n",
    "ENCODING = \"euc-kr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(path, audio_ext='.pcm', trans_ext='.txt'):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    \n",
    "    file_list = []\n",
    "    for _path, _dir, _files in os.walk(path):\n",
    "        for f in _files:\n",
    "            if f[-len(audio_ext):] == audio_ext:\n",
    "                f_name = os.path.join(_path, f[:-len(audio_ext)])\n",
    "                if os.path.exists(f_name + trans_ext):\n",
    "                    file_list.append(f_name)\n",
    "    file_list.sort()\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "def get_transcript(fname, trans_ext='.txt', ENCODING='euc-kr'):\n",
    "    txt = \"\"\n",
    "    try:\n",
    "        with open(fname + trans_ext, 'r', encoding=ENCODING) as f:\n",
    "            txt = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: {}\".format(e))\n",
    "    return txt\n",
    "\n",
    "def get_audiobuff(fname, audio_ext='.pcm', ENDIAN='int16'):\n",
    "    buff = None\n",
    "    try:\n",
    "        with open(fname + audio_ext, 'rb') as f:\n",
    "            buff = f.read()\n",
    "        buff = np.frombuffer(buff, dtype=ENDIAN)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: {}\".format(e))\n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Audio format.     : .pcm\n",
      " Transcript format : .txt\n",
      " Number of files   : 100\n",
      " Head of file list\n",
      "    ../data/KsponSpeech_sample/KsponSpeech_000001\n",
      "    ../data/KsponSpeech_sample/KsponSpeech_000002\n",
      "    ../data/KsponSpeech_sample/KsponSpeech_000003\n",
      "    ../data/KsponSpeech_sample/KsponSpeech_000004\n",
      "    ../data/KsponSpeech_sample/KsponSpeech_000005\n"
     ]
    }
   ],
   "source": [
    "file_list = get_file_list(data_dir, audio_ext, trans_ext)\n",
    "\n",
    "print(\" Audio format.     : {}\".format(audio_ext))\n",
    "print(\" Transcript format : {}\".format(trans_ext))\n",
    "print(\" Number of files   : {}\".format(len(file_list)))\n",
    "print(\" Head of file list\")\n",
    "for f in file_list[:5]:\n",
    "    print(\"   \", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded trans : 아/ 몬 소리야, 그건 또. b/\n",
      " Loaded audio length : 50368\n"
     ]
    }
   ],
   "source": [
    "sample_txt = get_transcript(file_list[0])\n",
    "sample_audio = get_audiobuff(file_list[0])\n",
    "\n",
    "print(\" Loaded trans : {}\".format(sample_txt))\n",
    "print(\" Loaded audio length : {}\".format(len(sample_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Transcript\n",
    "<b> Noise </b><br>\n",
    "- b/  : breath   -> 숨소리\n",
    "- n/  : noise    -> 노이즈\n",
    "- l/  : laugh    -> 웃음소리\n",
    "- o/  : occlude  -> 다른 사람의 목소리가 섞여있을 때\n",
    "\n",
    "<b> Number </b><br>\n",
    "ex)\n",
    "- (5대)/(오 대) 그룹이 모여, 자동차 (5대)/(다섯 대)를\n",
    "- (24시간)/(이십 사 시간), (24시간)/(스물 네 시간) -(867-860-2437)/(팔 육 칠 팔 육 공 에 이 사 삼 칠)\n",
    "- (14시)/(십 사 시), (14시)/(열 네 시)부터\n",
    "- (1999년)/(천 구백 구십 구 년)에, (1999년)/(일천 구백 구십 구 년)에\n",
    "\n",
    "<b> ETC </b><br>\n",
    "- '+' : 중복 발성     [AMBIG:DUP]\n",
    "- '*' : 불분명한 발성  [AMBIG:UNK]\n",
    "- 뭐/, 음/, 아/ 등과 같이 [한글]/ 형태의 경우 간투어를 의미함  [GANTU]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transcript Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   아/ 몬 소리야, 그건 또. b/\n",
      "   나는 악습은 원래 없어진다+ 없어져야 된다고 생각하긴 했는데 근데 그/ 약간 필요악으로 하나 정도쯤은 있어야 되거든. 물 뜨러 가고.\n",
      "   b/ n/ 그래서 지호랑 계단 n/ 올라와서 b/ 막 위에 운동하는 기구 있대요. b/ 그서 그걸로 운동 할려구요. b/ n/\n",
      "   뭐/ 정신과 병원도 그 약 타서 먹어보고, 그 한동안 연락이 안 된 적이 있었단 말이야. 그때가 언제였 언제였더라?\n",
      "   o/ b/ 그게 (0.1프로)/(영 점 일 프로) 가정의 아이들과 가정의 모습이야? b/\n",
      "   그/ 친애하는 판사님께라는 법+ 법 관련 드라마 알고 있어?\n",
      "   o/ 그래가지고 진짜 차 사야겠다 아니 뭐/ 차 안 되면 스쿠터라도 타야되겠다 막/ 그런 생각 들더라구 그래서 운전은 하는 게 좋은 거 같애 진짜 b/\n",
      "   그래\n",
      "   o/ 나도 몰라. 나 그/ (3G)/(쓰리 쥐)* 하나도 안 봤음. 어.\n",
      "   아/ 내일 나 알바하구나.\n",
      "   n/ 아/ 근데 (1시)/(한 시)에 닫는 게 쫌 아쉽긴 한데 거기 진짜 괜찮은데 b/\n",
      "   맞아. 시간 안에 풀고 약간 이것부터 풀고 요런 식으로 풀어보렴. 이런 거 b/\n",
      "   삼 점대야? 언니가?\n",
      "   한+ 한+ 한 시간에 이 만 원? 거의 이 정도로 이 정도란 말이야. b/\n",
      "   굳이 싶기도 해. 까* 오히려 사원 이런 것보다는 b/ 대신 돔* 먹+ 먹, 음식 거리, 먹거리가 우리 입맛에 좀 더 맞는 거 같고.\n"
     ]
    }
   ],
   "source": [
    "for file in file_list[:15]:\n",
    "    sample_txt = get_transcript(file)\n",
    "    print(\"  \", sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ b/ 그게 (0.1프로)/(영 점 일 프로)와 (15,3프로)/(십오 점 삼 프로) 가정의 아이들과 가정의 모습이야? b/\n"
     ]
    }
   ],
   "source": [
    "sample_txt = \"o/ b/ 그게 (0.1프로)/(영 점 일 프로)와 (15,3프로)/(십오 점 삼 프로) 가정의 아이들과 가정의 모습이야? b/\"\n",
    "print(sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(0.1프로)/(영 점 일 프로)', '(15,3프로)/(십오 점 삼 프로)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = re.findall(\"\\(.+?\\)/\\(.+?\\)\", sample_txt)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normStep1(txt):\n",
    "    # (0.1프로)/(영 점 일 프로) or (3G)/(쓰리 쥐) or (1시)/(한 시) 등과 같은\n",
    "    # (실제 표기)/(발음 표기) 형태의 데이터를 발음 표기의 데이터로 치환하는 함수\n",
    "    args = re.findall(\"\\(.+?\\)/\\(.+?\\)\", txt)\n",
    "    for arg in args:\n",
    "        _arg = re.sub(\"\\(.+?\\)/\", \"\", arg)\n",
    "        _arg = re.sub(\"[\\(\\)]\", \"\", _arg)\n",
    "        txt = txt.replace(arg, _arg)\n",
    "    return txt\n",
    "\n",
    "def normStep2(txt):\n",
    "    # . , ? 등 특수문자 제거\n",
    "    txt = re.sub(\"[.,?]\", \"\", txt)\n",
    "    return txt\n",
    "\n",
    "def normStep3(txt):\n",
    "    # noise symbol을 [NOISE:o], [NOIST:b] 등과 같이 변환\n",
    "    symbol_list = re.findall(\"[a-z]/\", txt)\n",
    "    for symbol in symbol_list:\n",
    "        _symbol = symbol.replace('/', '')\n",
    "        new_symbol = \"[NOISE:{}]\".format(_symbol)\n",
    "        txt = txt.replace(symbol, new_symbol)\n",
    "    return txt\n",
    "\n",
    "def normStep4(txt):\n",
    "    # +, *와 같은 불분명한 symbol에 대한 전처리\n",
    "    txt = re.sub(\"[\\+]\", \"[AMBIG:DUP]\", txt)\n",
    "    txt = re.sub(\"[\\*]\", \"[AMBIG:UNK]\", txt)\n",
    "    return txt\n",
    "    \n",
    "def normStep5(txt):\n",
    "    # 뭐/ 아/ 와 같은 간투어 처리\n",
    "    for _txt in txt.split():\n",
    "        symbol_list = re.findall(\"[가-힣].*?/\", _txt)\n",
    "        for symbol in symbol_list:\n",
    "            new_symbol = symbol.replace('/', '[GANTU]')\n",
    "            txt = txt.replace(symbol, new_symbol)\n",
    "    return txt\n",
    "\n",
    "def normalize(txt):\n",
    "    # step1 : to pronunciation\n",
    "    txt = normStep1(txt)\n",
    "    # step2 : erase special symbol\n",
    "    txt = normStep2(txt)\n",
    "    # step3 : noise tagging\n",
    "    txt = normStep3(txt)\n",
    "    # step4 : ambiguous symbol tagging\n",
    "    txt = normStep4(txt)\n",
    "    # step5 : tagging for GANTU symbol\n",
    "    txt = normStep5(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 'euc_kr' codec can't decode byte 0x98 in position 68: illegal multibyte sequence\n"
     ]
    }
   ],
   "source": [
    "txt_norm_list = []\n",
    "for file in file_list:\n",
    "    txt = get_transcript(file)\n",
    "    txt_norm = normalize(txt)\n",
    "    \n",
    "    txt_norm_list.append(txt_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0] input : 아/ 몬 소리야, 그건 또. b/\n",
      "     norm  : 아[GANTU] 몬 소리야 그건 또 [NOISE:b]\n",
      "\n",
      "[ 1] input : 나는 악습은 원래 없어진다+ 없어져야 된다고 생각하긴 했는데 근데 그/ 약간 필요악으로 하나 정도쯤은 있어야 되거든. 물 뜨러 가고.\n",
      "     norm  : 나는 악습은 원래 없어진다[AMBIG:DUP] 없어져야 된다고 생각하긴 했는데 근데 그[GANTU] 약간 필요악으로 하나 정도쯤은 있어야 되거든 물 뜨러 가고\n",
      "\n",
      "[ 2] input : b/ n/ 그래서 지호랑 계단 n/ 올라와서 b/ 막 위에 운동하는 기구 있대요. b/ 그서 그걸로 운동 할려구요. b/ n/\n",
      "     norm  : [NOISE:b] [NOISE:n] 그래서 지호랑 계단 [NOISE:n] 올라와서 [NOISE:b] 막 위에 운동하는 기구 있대요 [NOISE:b] 그서 그걸로 운동 할려구요 [NOISE:b] [NOISE:n]\n",
      "\n",
      "[ 3] input : 뭐/ 정신과 병원도 그 약 타서 먹어보고, 그 한동안 연락이 안 된 적이 있었단 말이야. 그때가 언제였 언제였더라?\n",
      "     norm  : 뭐[GANTU] 정신과 병원도 그 약 타서 먹어보고 그 한동안 연락이 안 된 적이 있었단 말이야 그때가 언제였 언제였더라\n",
      "\n",
      "[ 4] input : o/ b/ 그게 (0.1프로)/(영 점 일 프로) 가정의 아이들과 가정의 모습이야? b/\n",
      "     norm  : [NOISE:o] [NOISE:b] 그게 영 점 일 프로 가정의 아이들과 가정의 모습이야 [NOISE:b]\n",
      "\n",
      "[ 5] input : 그/ 친애하는 판사님께라는 법+ 법 관련 드라마 알고 있어?\n",
      "     norm  : 그[GANTU] 친애하는 판사님께라는 법[AMBIG:DUP] 법 관련 드라마 알고 있어\n",
      "\n",
      "[ 6] input : o/ 그래가지고 진짜 차 사야겠다 아니 뭐/ 차 안 되면 스쿠터라도 타야되겠다 막/ 그런 생각 들더라구 그래서 운전은 하는 게 좋은 거 같애 진짜 b/\n",
      "     norm  : [NOISE:o] 그래가지고 진짜 차 사야겠다 아니 뭐[GANTU] 차 안 되면 스쿠터라도 타야되겠다 막[GANTU] 그런 생각 들더라구 그래서 운전은 하는 게 좋은 거 같애 진짜 [NOISE:b]\n",
      "\n",
      "[ 7] input : 그래\n",
      "     norm  : 그래\n",
      "\n",
      "[ 8] input : o/ 나도 몰라. 나 그/ (3G)/(쓰리 쥐)* 하나도 안 봤음. 어.\n",
      "     norm  : [NOISE:o] 나도 몰라 나 그[GANTU] 쓰리 쥐[AMBIG:UNK] 하나도 안 봤음 어\n",
      "\n",
      "[ 9] input : 아/ 내일 나 알바하구나.\n",
      "     norm  : 아[GANTU] 내일 나 알바하구나\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(file_list[:10]):\n",
    "    txt = get_transcript(file)\n",
    "    txt_norm = normalize(txt)\n",
    "    \n",
    "    print(\"[{0:2d}] input : {1}\".format(i, txt))\n",
    "    print(\"     norm  : {}\".format(txt_norm))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Convert normalized transcript to lexical pronunciation symbol using G2P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')   # To import parent dir's packages\n",
    "\n",
    "from g2p import g2p\n",
    "ver_info = sys.version_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define Hangul / Symbols / Meta tags : for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANGULS = ['ㅂ', 'ㅍ', 'ㅃ', 'ㄷ', 'ㅌ', 'ㄸ', 'ㄱ', 'ㅋ', 'ㄲ', 'ㅅ', 'ㅆ', 'ㅎ', 'ㅈ', 'ㅊ', 'ㅉ', 'ㅁ', 'ㄴ', 'ㄹ', 'ㅂ', 'ㅍ', 'ㄷ', 'ㅌ', 'ㄱ', 'ㅋ', 'ㄲ', 'ㅅ', 'ㅆ', 'ㅎ', 'ㅈ', 'ㅊ', 'ㅁ', 'ㄴ', 'ㅇ', 'ㄹ', 'ㄱㅅ', 'ㄴㅈ', 'ㄴㅎ', 'ㄹㄱ', 'ㄹㅁ', 'ㄹㅂ', 'ㄹㅅ', 'ㄹㅌ', 'ㄹㅍ', 'ㄹㅎ', 'ㅂㅅ', 'ㅣ', 'ㅔ', 'ㅐ', 'ㅏ', 'ㅡ', 'ㅓ', 'ㅜ', 'ㅗ', 'ㅖ', 'ㅒ', 'ㅑ', 'ㅕ', 'ㅠ', 'ㅛ', 'ㅟ', 'ㅚ', 'ㅙ', 'ㅞ', 'ㅘ', 'ㅝ', 'ㅢ']\n",
    "SYMBOLS = ['p0', 'ph', 'pp', 't0', 'th', 'tt', 'k0', 'kh', 'kk', 's0', 'ss', 'h0', 'c0', 'ch', 'cc', 'mm', 'nn', 'rr', 'pf', 'ph', 'tf', 'th', 'kf', 'kh', 'kk', 's0', 'ss', 'h0', 'c0', 'ch', 'mf', 'nf', 'ng', 'll', 'ks', 'nc', 'nh', 'lk', 'lm', 'lb', 'ls', 'lt', 'lp', 'lh', 'ps', 'ii', 'ee', 'qq', 'aa', 'xx', 'vv', 'uu', 'oo', 'ye', 'yq', 'ya', 'yv', 'yu', 'yo', 'wi', 'wo', 'wq', 'we', 'wa', 'wv', 'xi']\n",
    "META_TAGS = ['[AMBIG:DUP]', '[AMBIG:UNK]', '[GANTU]', '[NOISE:b]', '[NOISE:n]', '[NOISE:l]', '[NOISE:o]', '[SPACE]', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Symbol <-> Label indexing을 위한 Mapping Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hangul</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ㅂ</td>\n",
       "      <td>p0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ㅍ</td>\n",
       "      <td>ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ㅃ</td>\n",
       "      <td>pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ㄷ</td>\n",
       "      <td>t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ㅌ</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hangul Symbol\n",
       "0      ㅂ     p0\n",
       "1      ㅍ     ph\n",
       "2      ㅃ     pp\n",
       "3      ㄷ     t0\n",
       "4      ㅌ     th"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_korSym = pd.DataFrame()\n",
    "\n",
    "df_korSym['Hangul'] = HANGULS\n",
    "df_korSym['Symbol'] = SYMBOLS\n",
    "for tag in META_TAGS:\n",
    "    df_korSym.loc[len(df_korSym)] = [tag, tag]\n",
    "df_korSym.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions for prepare korean to pronunciation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor2pron(txt, rule_in, rule_out, special_symbol='|'):\n",
    "    pron_list = []\n",
    "    for w in txt.split():\n",
    "        tag = re.findall('(\\[.+?\\])', w)\n",
    "        if tag:\n",
    "            w_base = w.replace(tag[0], '')\n",
    "            prons = g2p.graph2prono(w_base, rule_in, rule_out)\n",
    "            pron_list.append(special_symbol+prons+' '+tag[0])\n",
    "        else:\n",
    "            prons = g2p.graph2prono(w, rule_in, rule_out)\n",
    "            pron_list.append(special_symbol+prons)\n",
    "    pronun_txt = \" \".join(pron_list)\n",
    "    \n",
    "    return pronun_txt\n",
    "\n",
    "def pron_to_label(pron_txt, df_korSym, unk_flag=True):\n",
    "    pron_txt = pron_txt[1:] if pron_txt.startswith('|') else pron_txt\n",
    "    pron_txt = pron_txt.replace('|', '[SPACE] ')\n",
    "    \n",
    "    idx_list = []\n",
    "    for tag in pron_txt.split():\n",
    "        idx = df_korSym.index[df_korSym['Symbol'] == tag].tolist()[0]\n",
    "        idx_list.append(idx)\n",
    "    \n",
    "    unk_idx = df_korSym.index[df_korSym['Symbol'] == '[UNK]'].tolist()[0]\n",
    "    if unk_flag:\n",
    "        idx_list = [unk_idx] + idx_list + [unk_idx]\n",
    "    return idx_list\n",
    "\n",
    "def label_to_pron(idx_list, df_korSym, with_space=True, unk_flag=True):\n",
    "    pron_txt_list = []\n",
    "    for idx in idx_list:\n",
    "        pron = df_korSym.loc[idx]['Symbol']\n",
    "        pron_txt_list.append(pron)\n",
    "    if unk_flag:\n",
    "        pron_txt_list = pron_txt_list[1:-1]\n",
    "    \n",
    "    pron_txt = \" \".join(pron_txt_list)\n",
    "    pron_txt = pron_txt.replace('[SPACE] ', '|')\n",
    "    pron_txt = '|' + pron_txt\n",
    "    return pron_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate pronunciation with g2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rulebook using g2p\n",
    "[rule_in, rule_out] = g2p.readRules(ver_info[0], '../g2p/rulebook.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "> Origin Txt  : 아[GANTU] 몬 소리야 그건 또 [NOISE:b]\n",
      "  Pronon Txt  : |aa [GANTU] |mm oo nf |s0 oo rr ii ya |k0 xx k0 vv nf |tt oo | [NOISE:b]\n",
      "  Labels      : [74, 48, 68, 73, 15, 52, 31, 73, 9, 52, 17, 45, 55, 73, 6, 49, 6, 50, 31, 73, 5, 52, 73, 69, 74]\n",
      "\n",
      "====================================================================================================\n",
      "> Origin Txt  : 나는 악습은 원래 없어진다[AMBIG:DUP] 없어져야 된다고 생각하긴 했는데 근데 그[GANTU] 약간 필요악으로 하나 정도쯤은 있어야 되거든 물 뜨러 가고\n",
      "  Pronon Txt  : |nn aa nn xx nf |aa kf ss xx p0 xx nf |wv ll rr qq |vv pf ss vv c0 ii nf t0 aa [AMBIG:DUP] |vv pf ss vv c0 yv ya |t0 wo nf t0 aa k0 oo |s0 qq ng k0 aa kh aa k0 ii nf |h0 qq nf nn xx nf t0 ee |k0 xx nf t0 ee |k0 xx [GANTU] |ya kf kk aa nf |ph ii ll rr yo aa k0 xx rr oo |h0 aa nn aa |c0 vv ng t0 oo cc xx mm xx nf |ii ss vv ya |t0 wo k0 vv t0 xx nf |mm uu ll |tt xx rr vv |k0 aa k0 oo\n",
      "  Labels      : [74, 16, 48, 16, 49, 31, 73, 48, 22, 10, 49, 0, 49, 31, 73, 64, 33, 17, 47, 73, 50, 18, 10, 50, 12, 45, 31, 3, 48, 66, 73, 50, 18, 10, 50, 12, 56, 55, 73, 3, 60, 31, 3, 48, 6, 52, 73, 9, 47, 32, 6, 48, 7, 48, 6, 45, 31, 73, 11, 47, 31, 16, 49, 31, 3, 46, 73, 6, 49, 31, 3, 46, 73, 6, 49, 68, 73, 55, 22, 8, 48, 31, 73, 1, 45, 33, 17, 58, 48, 6, 49, 17, 52, 73, 11, 48, 16, 48, 73, 12, 50, 32, 3, 52, 14, 49, 15, 49, 31, 73, 45, 10, 50, 55, 73, 3, 60, 6, 50, 3, 49, 31, 73, 15, 51, 33, 73, 5, 49, 17, 50, 73, 6, 48, 6, 52, 74]\n",
      "\n",
      "====================================================================================================\n",
      "> Origin Txt  : [NOISE:b] [NOISE:n] 그래서 지호랑 계단 [NOISE:n] 올라와서 [NOISE:b] 막 위에 운동하는 기구 있대요 [NOISE:b] 그서 그걸로 운동 할려구요 [NOISE:b] [NOISE:n]\n",
      "  Pronon Txt  : | [NOISE:b] | [NOISE:n] |k0 xx rr qq s0 vv |c0 ii h0 oo rr aa ng |k0 ye t0 aa nf | [NOISE:n] |oo ll rr aa wa s0 vv | [NOISE:b] |mm aa kf |wi ee |uu nf t0 oo ng h0 aa nn xx nf |k0 ii k0 uu |ii tf tt qq yo | [NOISE:b] |k0 xx s0 vv |k0 xx k0 vv ll rr oo |uu nf t0 oo ng |h0 aa ll rr yv k0 uu yo | [NOISE:b] | [NOISE:n]\n",
      "  Labels      : [74, 69, 73, 70, 73, 6, 49, 17, 47, 9, 50, 73, 12, 45, 11, 52, 17, 48, 32, 73, 6, 53, 3, 48, 31, 73, 70, 73, 52, 33, 17, 48, 63, 9, 50, 73, 69, 73, 15, 48, 22, 73, 59, 46, 73, 51, 31, 3, 52, 32, 11, 48, 16, 49, 31, 73, 6, 45, 6, 51, 73, 45, 20, 5, 47, 58, 73, 69, 73, 6, 49, 9, 50, 73, 6, 49, 6, 50, 33, 17, 52, 73, 51, 31, 3, 52, 32, 73, 11, 48, 33, 17, 56, 6, 51, 58, 73, 69, 73, 70, 74]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pron_txt_list = [kor2pron(txt, rule_in, rule_out) for txt in txt_norm_list]\n",
    "\n",
    "for txt in txt_norm_list[:3]:\n",
    "    pron_txt = kor2pron(txt, rule_in, rule_out)\n",
    "    labels = pron_to_label(pron_txt, df_korSym, True)\n",
    "    print(\"=\"*100)\n",
    "    print(\"> Origin Txt  : {}\".format(txt))\n",
    "    print(\"  Pronon Txt  : {}\".format(pron_txt))\n",
    "    print(\"  Labels      : {}\".format(labels))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pron_txt = pron_txt_list[0]\n",
    "# txt_norm = txt_norm_list[0]\n",
    "\n",
    "# print(\" Normalized text : \", txt_norm)\n",
    "# print(\" Pronunciation   : \", pron_txt)\n",
    "# print()\n",
    "\n",
    "# word_prons = pron_txt.split('|')\n",
    "# decode_ch_list = []\n",
    "# for word_pron in word_prons:\n",
    "#     decode_ch_list.append('|')\n",
    "#     for symbol in word_pron.split():\n",
    "#         decode_ch_list.append(df[df[\"Symbol\"] == symbol][\"Hangul\"].iloc[0])\n",
    "\n",
    "# decoded_txt = \"\".join(decode_ch_list)\n",
    "# decoded_txt = decoded_txt.replace(\"|\", \" \")\n",
    "# print(decode_ch_list)\n",
    "# print(decoded_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
